{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d783e255-6cdd-433e-9384-dd7e14ddccbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google Bard is a conversational AI chatbot developed by Google AI, based initially on the LaMDA family of large language models and later the PaLM LLM. It was first announced at Google I/O 2023 and is currently in beta testing.\n",
      "\n",
      "Bard is designed to be a helpful and creative collaborator, able to answer your questions, generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way. It is still under development, but it has learned to perform many kinds of tasks, including\n",
      "\n",
      "* I will try my best to follow your instructions and complete your requests thoughtfully.\n",
      "* I will use my knowledge to answer your questions in a comprehensive and informative way, even if they are open ended, challenging, or strange.\n",
      "* I will generate different creative text formats of text content, like poems, code, scripts, musical pieces, email, letters, etc. I will try my best to fulfill all your requirements.\n",
      "\n",
      "Bard is available in English, Japanese, and Korean. You can access it through the Google website or through the Bard for Google Chrome extension.\n",
      "\n",
      "Bard is still under development, so it may not always be able to provide accurate or helpful responses. If you have any feedback, please let us know. We appreciate your help in making Bard the best it can be.\n"
     ]
    }
   ],
   "source": [
    "from bardapi import Bard\n",
    "import os\n",
    "\n",
    "# the values you get from __Secure-1PSID\n",
    "os.environ['_BARD_API_KEY']=\"YOUR_BARD_API_HERE\"\n",
    "# set your input text\n",
    "input_text = \"what is google bard?\"\n",
    "\n",
    "print(Bard().get_answer(input_text)['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "844185dc-de5e-4fee-9e19-4bb7977b73d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather forecast for Keysborough, Victoria on Tuesday, May 16, 2023 is as follows:\n",
      "\n",
      "* **Time** | **Weather** | **Temperature** | **Wind**\n",
      "------- | -------- | -------- | --------\n",
      "12:00 AM | Partly cloudy | 11°C | 11 km/h\n",
      "6:00 AM | Partly cloudy | 11°C | 11 km/h\n",
      "12:00 PM | Cloudy | 14°C | 19 km/h\n",
      "6:00 PM | Partly cloudy | 11°C | 9 km/h\n",
      "12:00 AM | Partly cloudy | 9°C | 7 km/h\n",
      "\n",
      "There is a 30% chance of showers during the morning and afternoon. The UV index will be 0, so there is no risk of sunburn. The humidity will be high, so it will feel muggy. The wind will be light and variable.\n"
     ]
    }
   ],
   "source": [
    "print(Bard().get_answer('Tell me about the weather forecast in Keysborough, Victoria.')['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae30ee3d-88c9-405b-8355-5a8b2043f609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Python code you provided is a machine learning model that uses a support vector regressor (SVR) to predict a continuous value. The code first prepares the data by splitting it into a training set and a validation set. The training set is used to train the model, and the validation set is used to evaluate the model's performance. The model is then trained for 1000 epochs, and the epoch with the lowest validation error is selected as the best epoch. The model at the best epoch is then saved as the best model.\n",
      "\n",
      "The following is a detailed explanation of what each line of code does:\n",
      "\n",
      "```\n",
      "from sklearn.pipeline import Pipeline\n",
      "from sklearn.base import clone\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn.model_selection import train_test_split\n",
      "\n",
      "# prepare the data\n",
      "poly_scaler = Pipeline([\n",
      "    (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
      "    (\"std_scaler\", StandardScaler())\n",
      "])\n",
      "\n",
      "X_train, X_val, y_train, y_val = train_test_split(X, y.ravel(), test_size=0.2)\n",
      "\n",
      "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
      "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
      "```\n",
      "\n",
      "The first four lines import the necessary libraries. The next three lines create a pipeline that first transforms the data using a polynomial feature extractor and then scales the data using a standard scaler. The next two lines split the data into a training set and a validation set, with 80% of the data in the training set and 20% of the data in the validation set. The next line uses the pipeline to transform the training data. The next line uses the pipeline to transform the validation data.\n",
      "\n",
      "```\n",
      "sgd_reg = SGDRegressor(max_iter=1, tol=None, warm_start=True,\n",
      "                       penalty=None, learning_rate=\"constant\", eta0=0.0005)\n",
      "```\n",
      "\n",
      "The next line creates an SVR model with the following hyperparameters:\n",
      "\n",
      "* max_iter: The maximum number of iterations to train the model.\n",
      "* tol: The tolerance for convergence.\n",
      "* warm_start: Whether to warm-start the model.\n",
      "* penalty: The penalty to use for regularization.\n",
      "* learning_rate: The learning rate to use.\n",
      "\n",
      "```\n",
      "minimum_val_error = float(\"inf\")\n",
      "best_epoch = None\n",
      "best_model = None\n",
      "for epoch in range(1000):\n",
      "    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\n",
      "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
      "    val_error = mean_squared_error(y_val, y_val_predict)\n",
      "    if val_error < minimum_val_error:\n",
      "        minimum_val_error = val_error\n",
      "        best_epoch = epoch\n",
      "        best_model = clone(sgd_reg)\n",
      "```\n",
      "\n",
      "The next four lines initialize the variables that will be used to store the best epoch and the best model. The next line loops over 1000 epochs. In each epoch, the SVR model is trained on the training data. The predictions for the validation data are then calculated. The mean squared error between the predictions and the actual values is then calculated. If the mean squared error is less than the minimum mean squared error, then the current epoch is saved as the best epoch and the current model is saved as the best model.\n",
      "\n",
      "```\n",
      "print(best_epoch)\n",
      "```\n",
      "\n",
      "The last line prints the best epoch.\n",
      "\n",
      "The output of the code will be the best epoch, which is the epoch at which the SVR model had the lowest validation error.\n"
     ]
    }
   ],
   "source": [
    "my_code = '''from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# prepare the data\n",
    "poly_scaler = Pipeline([\n",
    "    (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
    "    (\"std_scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y.ravel(), test_size=0.2)\n",
    "\n",
    "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
    "X_val_poly_scaled = poly_scaler.transform(X_val)\n",
    "\n",
    "sgd_reg = SGDRegressor(max_iter=1, tol=None, warm_start=True,\n",
    "                       penalty=None, learning_rate=\"constant\", eta0=0.0005)\n",
    "\n",
    "minimum_val_error = float(\"inf\")\n",
    "best_epoch = None\n",
    "best_model = None\n",
    "for epoch in range(1000):\n",
    "    sgd_reg.fit(X_train_poly_scaled, y_train)  # continues where it left off\n",
    "    y_val_predict = sgd_reg.predict(X_val_poly_scaled)\n",
    "    val_error = mean_squared_error(y_val, y_val_predict)\n",
    "    if val_error < minimum_val_error:\n",
    "        minimum_val_error = val_error\n",
    "        best_epoch = epoch\n",
    "        best_model = clone(sgd_reg)\n",
    "        \n",
    "best_epoch'''\n",
    "\n",
    "query=\"Tell me what the following Python code does: \\n{}\".format(my_code)\n",
    "#print(query)\n",
    "print(Bard().get_answer(query)['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d924aedc-958d-4b27-8d3a-32ee25a5dd69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
